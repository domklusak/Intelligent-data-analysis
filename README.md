# Intelligent Data Analysis Project

## Overview
This project, was part of an Intelligent Data Analysis subject, involves a detailed analysis of a weather dataset. Conducted in collaboration, it is structured into three distinct phases, each focusing on different aspects of data handling and analysis.

## Phases
### Phase 1: Data Exploration and Hypothesis Formulation
- **Objective**: Familiarization with the dataset, focusing on weather-related data.
- **Activities**: 
  - Initial data inspection to identify interesting aspects.
  - Merging weather data with location information.
  - Comprehensive data description and visualization.
  - Performing correlation analysis.
  - Formulating two hypotheses based on initial observations.
- **Tools Used**: 
  - `pandas`: For data manipulation and analysis.
  - `seaborn`: Advanced data visualization.
  - `matplotlib.pyplot`: Basic plotting and visualization.
  - `scipy`: For scientific and technical computing.
  - `numpy`: Support for large, multi-dimensional arrays and matrices.
  - `statsmodels.api`: Statistical models, tests, and data exploration.
  - `sklearn.preprocessing.MinMaxScaler`: Feature scaling.

### Phase 2: Data Processing and Integration
- **Objective**: Clean and integrate the dataset for advanced analysis.
- **Activities**: 
  - Detailed data cleaning to improve quality and consistency.
  - Integration of various data sources for a more comprehensive dataset.
  - Ensuring the reproducibility of data preprocessing.
- **Tools Used**:
  - `pandas`: Data manipulation and analysis.
  - `seaborn`: Data visualization.
  - `sklearn.base`: Base classes for all estimators and transformer mixins.
  - `matplotlib.pyplot`: Plotting library.
  - `scipy`: Scientific computing.
  - `numpy`: Multi-dimensional arrays and matrices.
  - `statsmodels.api`: Statistical models and tests.
  - `sklearn.preprocessing`: Data preprocessing with MinMaxScaler and StandardScaler.
  - `sklearn.pipeline`: Pipeline tools for chaining estimators.
  - `sklearn.impute.KNNImputer`: Imputation for completing missing values.

### Phase 3: Machine Learning Implementation
- **Objective**: Apply machine learning techniques to the processed data.
- **Activities**: 
  - Developing a simple classifier to explore dependencies in the data.
  - Training and evaluating various machine learning classifiers.
  - Optimization of the classifiers through hyperparameter tuning.
- **Tools Used**:
  - `pandas`: For data manipulation.
  - `numpy`: Array and matrix operations.
  - `sklearn.model_selection`: Tools for model selection, including train/test splits and grid/randomized search.
  - `sklearn.ensemble.RandomForestClassifier`: Random forest classifier.
  - `sklearn.metrics`: Metrics for model evaluation (accuracy, recall, precision, etc.).
  - `sklearn.tree.DecisionTreeClassifier`: Decision tree classifier.
  - `matplotlib.pyplot`: Plotting and visualization.
  - `IPython.display`: Displaying images and outputs in Jupyter.
  - `os`: Interfacing with the operating system.
  - `graphviz`, `pydot`: Visualization of decision trees.

## Team Members
- This project was made with my colleague Richard Kello

## Usage
- Each Jupyter Notebook (`.ipynb`) file corresponds to a phase of the project.
- Follow the instructions and comments within each notebook for a guided analysis process.

## Additional Information
For more detailed descriptions of each phase, including methodologies, analysis techniques, and findings, refer to the individual Jupyter Notebooks provided.
